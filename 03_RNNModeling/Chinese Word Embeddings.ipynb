{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Darwin'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import string\n",
    "from fastText import load_model\n",
    "import platform\n",
    "platform.system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwe = np.load(\"zh.bin.syn0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50101, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwe_neg = np.load(\"zh.bin.syn1neg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50101, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwe_neg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Matrix Lookup (cross platform)\n",
    "Reference: [https://github.com/facebookresearch/fastText/tree/master/python](https://github.com/facebookresearch/fastText/tree/master/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    with open(\"zh/wiki.zh.vec\", \"r\", encoding='utf-8') as fin:\n",
    "        words = fin.readlines()\n",
    "elif platform.system() == 'Darwin':  # Mac OSX\n",
    "    with open(\"zh/wiki.zh.vec\", \"r\") as fin:\n",
    "        words = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332648"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    wiki_vocab = [x.split(' ')[0] for x in words[1:]]\n",
    "elif platform.system() == 'Darwin':\n",
    "    wiki_vocab = [x.split(' ')[0].decode('utf-8') for x in words[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332647"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本\n"
     ]
    }
   ],
   "source": [
    "print(wiki_vocab[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wiki_embeddings = np.array([x.strip().split(' ')[1:] for x in words[1:]])  # will probably run out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vocab(file_path):\n",
    "    if platform.system() == 'Windows':\n",
    "        with open(file_path, 'r', encoding='utf-8') as fin:\n",
    "            vocab = [x.strip() for x in fin.readlines()]\n",
    "            vocab = [x for x in vocab if x != '']\n",
    "    elif platform.system() == 'Darwin':\n",
    "        with open(file_path, 'r') as fin:\n",
    "            vocab = [x.strip() for x in fin.readlines()]\n",
    "            vocab = [x.decode('utf-8') for x in vocab if x != '']\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = read_vocab('../01_WebScraping/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9827"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与我无关\n"
     ]
    }
   ],
   "source": [
    "print(vocab[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return [token for token in jieba.cut(text) if token not in string.punctuation+\" \" and token != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lyrics(file_path):\n",
    "    if platform.system() == 'Windows':\n",
    "        lyrics = [line.strip() for line in open(file_path, 'r', encoding='utf-8').readlines()]\n",
    "        lyrics = [custom_tokenizer(text) for text in lyrics]\n",
    "    elif platform.system() == 'Darwin':\n",
    "        lyrics = [line.strip() for line in open(file_path, 'r').readlines()]\n",
    "        lyrics = [custom_tokenizer(text) for text in lyrics]\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/45/9s5cv4kn143fggkxy4bdc89c0000gn/T/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /var/folders/45/9s5cv4kn143fggkxy4bdc89c0000gn/T/jieba.cache\n",
      "Loading model cost 3.991 seconds.\n",
      "DEBUG:jieba:Loading model cost 3.991 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "lyrics = read_lyrics('../01_WebScraping/lyrics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "210\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "for item in lyrics[:3]:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u8fd9\\u4e2a', u'\\u4e16\\u4e0a', u'\\u672c', u'\\u6ca1\\u6709', u'\\u8def', u'\\u8d70', u'\\u7684', u'\\u4eba', u'\\u591a', u'\\u4e86', u'\\u4e5f', u'\\u5c31', u'\\u6210', u'\\u4e86', u'\\u8def', u'\\u7ecf\\u8fc7', u'\\u90a3\\u4e9b', u'\\u8ff7\\u96fe', u'\\u7a7f\\u8fc7', u'\\u524d\\u4eba', u'\\u683d\\u4e0b', u'\\u7684', u'\\u6811', u'\\u4e8e\\u662f', u'\\u8fdb\\u5165', u'\\u8ba4\\u8f93', u'\\u6124\\u6012', u'\\u6211', u'\\u6e05\\u695a', u'\\u6211', u'\\u8d70', u'\\u5728', u'\\u8fd9\\u6761', u'\\u4e0d\\u5f52\\u8def', u'\\u4e3a', u'\\u6bcf\\u4e2a', u'\\u97f5\\u811a', u'\\u800c', u'\\u7275\\u80a0', u'\\u7231', u'\\u7684', u'\\u4e0d\\u662f', u'\\u82b1\\u9999', u'\\u6211', u'\\u6df1\\u7231\\u7740', u'\\u8fd9\\u7247', u'\\u5730\\u4e0b', u'\\u7eaf\\u771f', u'\\u7684', u'\\u571f\\u58e4', u'\\u575a\\u6301', u'\\u7740', u'\\u6839', u'\\u7684', u'\\u4fe1\\u4ef0', u'\\u4e00\\u5982\\u65e2\\u5f80', u'\\u591a\\u5c11', u'\\u65f6', u'\\u6ca1\\u4eba', u'\\u6b23\\u8d4f', u'\\u81f3\\u5c11', u'\\u662f', u'\\u5b64\\u82b3\\u81ea\\u8d4f', u'\\u5fd8\\u8bb0', u'\\u6211', u'\\u50cf', u'\\u4e2a', u'\\u5b69\\u5b50', u'\\u4e00\\u6837', u'\\u8654\\u8bda', u'\\u7684', u'\\u51dd\\u671b', u'\\u4f60', u'\\u80fd', u'\\u4e0d\\u80fd', u'\\u7ed9', u'\\u6211', u'\\u4e00\\u679d', u'\\u9ed1\\u591c', u'\\u4e2d', u'\\u4e66\\u5199', u'\\u8367\\u5149\\u7b14']\n"
     ]
    }
   ],
   "source": [
    "print(lyrics[:3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token look up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup(token, vocab, word_embedding):\n",
    "    try:\n",
    "        return [float(x) for x in word_embedding[vocab.index(token)+1].strip().split(' ')[1:]]\n",
    "    except ValueError:\n",
    "        print('Token not in vocabulary!')\n",
    "        # See https://github.com/facebookresearch/fastText#obtaining-word-vectors-for-out-of-vocabulary-words for more details\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lookup(vocab[5], wiki_vocab, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Skipgram model\n",
    "model = load_model('zh/wiki.zh.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.64944708e-01 -2.94708818e-01 -1.20754756e-01 -3.09936672e-01\n",
      "  6.37601092e-02 -2.40231514e-01 -3.71293902e-01  1.44274905e-01\n",
      " -9.92775857e-02  7.42849857e-02  2.17024520e-01  1.55438393e-01\n",
      "  4.50631678e-01  7.64232650e-02  2.16057256e-01  4.16888416e-01\n",
      "  8.26665610e-02 -8.16601142e-02 -5.49373738e-02 -1.18559822e-01\n",
      "  1.29962638e-01  2.90348232e-01 -3.01082432e-01 -1.81621977e-03\n",
      " -4.09690052e-01  1.76661704e-02 -2.81581312e-01  3.01812068e-02\n",
      "  4.43937391e-01  7.44513050e-02 -3.16726595e-01 -3.67642403e-01\n",
      " -6.18718080e-02 -1.29553545e-02 -4.01773304e-02 -1.17563143e-01\n",
      "  1.89177191e-03 -2.51461953e-01  6.98114336e-02  1.27828360e-01\n",
      " -1.36616215e-01 -1.81653649e-01 -7.80129433e-02  7.68107846e-02\n",
      " -1.58297732e-01  1.67796090e-01 -2.62066036e-01  2.58101169e-02\n",
      " -4.64072347e-01  1.38130531e-01 -4.89218570e-02  1.10212512e-01\n",
      " -8.34304914e-02 -3.01598728e-01  6.73357910e-03 -1.22148782e-01\n",
      " -1.99100763e-01 -5.83374836e-02  4.24957603e-01 -1.08874626e-01\n",
      "  1.69648170e-01  1.55079529e-01  2.75815073e-02 -8.27434734e-02\n",
      "  1.43580750e-01 -3.17741632e-01  1.90891713e-01  2.08629712e-01\n",
      " -2.77207136e-01 -1.54544367e-02 -9.96157750e-02 -2.18398482e-01\n",
      " -1.02229252e-01 -1.22878194e-01  2.49866638e-02  2.11282074e-01\n",
      "  1.06649268e-02 -3.11150581e-01 -3.93272042e-02 -1.28687739e-01\n",
      "  9.56951752e-02 -6.24203421e-02 -2.67275907e-02  4.94245440e-02\n",
      " -1.01993389e-01  2.86097914e-01  1.01312377e-01 -1.89317092e-01\n",
      "  7.15974951e-03 -7.82366022e-02  2.92644948e-01 -2.17198685e-01\n",
      "  1.43071115e-01 -1.96213201e-02  1.50961876e-01  7.23028705e-02\n",
      "  2.91553915e-01  2.68816382e-01  8.92578363e-02 -4.24329424e-03\n",
      "  2.72632241e-02  9.05871391e-02  4.73494157e-02  1.21190645e-01\n",
      "  3.73936057e-01  6.19055182e-02 -2.10294530e-01  2.87900627e-01\n",
      "  1.87582438e-04  1.10479146e-01  1.26502723e-01  2.26278096e-01\n",
      " -1.07915379e-01 -1.09926267e-02  2.38793846e-02  2.72830009e-01\n",
      " -2.16078106e-02  4.29597437e-01 -2.44733840e-01 -2.45815888e-01\n",
      "  1.64240729e-02  2.07797468e-01 -7.26357428e-03  4.12710756e-02\n",
      "  1.37341097e-01 -2.73919493e-01 -1.49248883e-01 -2.74609886e-02\n",
      "  2.56912917e-01 -1.22178473e-01 -5.94717339e-02  1.17857881e-01\n",
      "  5.46820424e-02 -1.39934137e-01 -4.04202156e-02  2.79059678e-01\n",
      " -8.44550207e-02  6.86392933e-02  2.45607331e-01  2.76864115e-02\n",
      "  1.60773903e-01 -1.40793160e-01 -6.47363141e-02 -2.70273034e-02\n",
      " -4.16940190e-02 -6.17701486e-02  1.06523909e-01 -1.56271696e-01\n",
      " -1.21952467e-01 -1.38624776e-02 -2.85106063e-01 -2.17906177e-01\n",
      " -2.45571330e-01 -1.20942630e-01  1.42932892e-01  1.05608381e-01\n",
      " -1.28706321e-01  3.54650281e-02  1.73031226e-01 -1.88875049e-01\n",
      "  1.19519368e-01 -9.16109458e-02 -4.19379631e-03  7.68214762e-02\n",
      "  4.45500910e-02 -2.96216547e-01 -3.80356535e-02  1.06930211e-01\n",
      "  2.58170199e-02 -2.88773049e-02  8.44898745e-02 -3.80144231e-02\n",
      " -4.23044749e-02  1.56736836e-01  1.91807747e-02  1.06248312e-01\n",
      " -3.93552542e-01  1.50520355e-01  5.45888208e-02  1.65625498e-01\n",
      "  5.42258024e-01 -1.02523714e-01 -1.80594876e-01  2.47235298e-02\n",
      " -3.81040461e-02  1.79277375e-01 -4.83999252e-02  1.66564249e-02\n",
      "  1.02324426e-01 -2.16356128e-01  1.15882978e-01 -1.84532940e-01\n",
      "  7.75757758e-03  1.04041837e-01  1.08533792e-01  1.78735763e-01\n",
      "  1.48913965e-01 -1.13274325e-02 -9.91452262e-02 -1.99729413e-01\n",
      " -1.50837556e-01  3.10788065e-01  6.14232309e-02  2.92549521e-01\n",
      "  2.83539146e-01 -2.20328450e-01 -1.70943946e-01  4.46421206e-01\n",
      "  3.63590270e-02  2.29621619e-01 -3.32883209e-01 -2.30968907e-01\n",
      "  3.37940939e-02 -2.24399958e-02 -1.50885254e-01 -5.14576770e-02\n",
      "  5.59979342e-02  2.24615648e-01  1.06735744e-01  4.41986918e-02\n",
      " -2.09795386e-01 -9.16298404e-02 -1.16028205e-01 -1.23615175e-01\n",
      "  1.08050801e-01  5.57114035e-02 -3.90239544e-02 -1.40618850e-02\n",
      "  1.02351107e-01 -1.43916294e-01  1.49669096e-01 -7.26824775e-02\n",
      "  1.36137947e-01 -6.64044097e-02  1.49872527e-01  1.47688985e-01\n",
      " -3.33148181e-01  8.38892981e-02 -8.84161592e-02 -1.41135961e-01\n",
      "  1.42898276e-01 -2.84279376e-01 -2.01552123e-01 -1.42701492e-01\n",
      "  1.56151727e-01  3.66134286e-01  5.99496961e-01  1.19794860e-01\n",
      "  7.83501044e-02 -3.87828887e-01 -1.79293036e-01 -2.49978036e-01\n",
      " -2.81297743e-01 -4.72211093e-01 -1.82703300e-03 -2.26673126e-01\n",
      "  2.14667365e-01 -1.56390011e-01  1.20579004e-01  2.49465093e-01\n",
      " -1.19958790e-02 -2.13214129e-01  1.08269297e-01  2.83169419e-01\n",
      "  3.37560982e-01  9.59303638e-04  1.87499061e-01  5.93447685e-01\n",
      " -1.51061833e-01 -3.60241264e-01 -5.95279813e-01 -2.45242894e-01\n",
      "  4.96138036e-01 -4.96312119e-02  1.27971888e-01 -7.73935318e-02\n",
      "  3.49052638e-01 -2.03946307e-01  1.76544860e-02 -2.62726754e-01\n",
      " -2.65697956e-01 -5.76341093e-01  1.23469211e-01  9.11044404e-02\n",
      " -1.99664071e-01  1.69905484e-01 -5.07855535e-01  8.85423571e-02\n",
      " -2.70081669e-01  1.85444698e-01 -3.91784698e-01  3.60300690e-01\n",
      " -1.37130961e-01 -2.61235505e-01 -9.00302231e-02  4.47945386e-01\n",
      "  1.31220132e-01  1.48779944e-01 -1.12675257e-01  2.99037904e-01]\n"
     ]
    }
   ],
   "source": [
    "print(model.get_word_vector(vocab[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on instance of _FastText in module fastText.FastText:\n",
      "\n",
      "class _FastText\n",
      " |  This class defines the API to inspect models and should not be used to\n",
      " |  create objects. It will be returned by functions such as load_model or\n",
      " |  train.\n",
      " |  \n",
      " |  In general this API assumes to be given only unicode for Python2 and the\n",
      " |  Python3 equvalent called str for any string-like arguments. All unicode\n",
      " |  strings are then encoded as UTF-8 and fed to the fastText C++ API.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model=None)\n",
      " |  \n",
      " |  get_dimension(self)\n",
      " |      Get the dimension (size) of a lookup vector (hidden layer).\n",
      " |  \n",
      " |  get_input_matrix(self)\n",
      " |      Get a copy of the full input matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_input_vector(self, ind)\n",
      " |      Given an index, get the corresponding vector of the Input Matrix.\n",
      " |  \n",
      " |  get_labels(self, include_freq=False)\n",
      " |      Get the entire list of labels of the dictionary optionally\n",
      " |      including the frequency of the individual labels. Unsupervised\n",
      " |      models use words as labels, which is why get_labels\n",
      " |      will call and return get_words for this type of\n",
      " |      model.\n",
      " |  \n",
      " |  get_line(self, text)\n",
      " |      Split a line of text into words and labels. Labels must start with\n",
      " |      the prefix used to create the model (__label__ by default).\n",
      " |  \n",
      " |  get_output_matrix(self)\n",
      " |      Get a copy of the full output matrix of a Model. This only\n",
      " |      works if the model is not quantized.\n",
      " |  \n",
      " |  get_sentence_vector(self, text)\n",
      " |      Given a string, get a single vector represenation. This function\n",
      " |      assumes to be given a single line of text. We split words on\n",
      " |      whitespace (space, newline, tab, vertical tab) and the control\n",
      " |      characters carriage return, formfeed and the null character.\n",
      " |  \n",
      " |  get_subword_id(self, subword)\n",
      " |      Given a subword, return the index (within input matrix) it hashes to.\n",
      " |  \n",
      " |  get_subwords(self, word)\n",
      " |      Given a word, get the subwords and their indicies.\n",
      " |  \n",
      " |  get_word_id(self, word)\n",
      " |      Given a word, get the word id within the dictionary.\n",
      " |      Returns -1 if word is not in the dictionary.\n",
      " |  \n",
      " |  get_word_vector(self, word)\n",
      " |      Get the vector representation of word.\n",
      " |  \n",
      " |  get_words(self, include_freq=False)\n",
      " |      Get the entire list of words of the dictionary optionally\n",
      " |      including the frequency of the individual words. This\n",
      " |      does not include any subwords. For that please consult\n",
      " |      the function get_subwords.\n",
      " |  \n",
      " |  is_quantized(self)\n",
      " |  \n",
      " |  predict(self, text, k=1, threshold=0.0)\n",
      " |      Given a string, get a list of labels and a list of\n",
      " |      corresponding probabilities. k controls the number\n",
      " |      of returned labels. A choice of 5, will return the 5\n",
      " |      most probable labels. By default this returns only\n",
      " |      the most likely label and probability. threshold filters\n",
      " |      the returned labels by a threshold on probability. A\n",
      " |      choice of 0.5 will return labels with at least 0.5\n",
      " |      probability. k and threshold will be applied together to\n",
      " |      determine the returned labels.\n",
      " |      \n",
      " |      This function assumes to be given\n",
      " |      a single line of text. We split words on whitespace (space,\n",
      " |      newline, tab, vertical tab) and the control characters carriage\n",
      " |      return, formfeed and the null character.\n",
      " |      \n",
      " |      If the model is not supervised, this function will throw a ValueError.\n",
      " |      \n",
      " |      If given a list of strings, it will return a list of results as usually\n",
      " |      received for a single line of text.\n",
      " |  \n",
      " |  quantize(self, input=None, qout=False, cutoff=0, retrain=False, epoch=None, lr=None, thread=None, verbose=None, dsub=2, qnorm=False)\n",
      " |      Quantize the model reducing the size of the model and\n",
      " |      it's memory footprint.\n",
      " |  \n",
      " |  save_model(self, path)\n",
      " |      Save the model to the given path\n",
      " |  \n",
      " |  test(self, path, k=1)\n",
      " |      Evaluate supervised model using file given by path\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [[model.get_word_vector(word) for word in sentence] for sentence in lyrics[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test[2][0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
